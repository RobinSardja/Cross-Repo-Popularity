{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad9b50a",
   "metadata": {},
   "source": [
    "# Split Main Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ddef2",
   "metadata": {},
   "source": [
    "Main Dataset https://www.kaggle.com/datasets/johntukey/github-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833c5c2",
   "metadata": {},
   "source": [
    "1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ea3ca",
   "metadata": {},
   "source": [
    "2. Split dataset into desired number of splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c357e46-616f-417a-9b1e-ee3e7d8d2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitMainDataset(infile, chunk_size=1000, total_lines=8356231, percent=5):\n",
    "    lines_to_process = int(total_lines * (percent / 100))\n",
    "    curr = 1\n",
    "    processed_lines = 0\n",
    "    first_chunk = True\n",
    "\n",
    "    outfile = \"split\" + str(curr) + \".json\"\n",
    "\n",
    "    for chunk in pd.read_json(infile, lines=True, chunksize=chunk_size):\n",
    "        # If adding the current chunk exceeds the target, trim the chunk\n",
    "        if processed_lines + chunk.shape[0] > lines_to_process:\n",
    "            chunk = chunk.iloc[:(lines_to_process - processed_lines)]\n",
    "\n",
    "        users = []\n",
    "        for _, row in chunk.iterrows():\n",
    "            \n",
    "            user_data = {\n",
    "                'login': row.get('login', ''),\n",
    "                'id': row.get('id', ''),\n",
    "                'type': row.get('type', ''),\n",
    "                'created_at': row.get('created_at', ''),\n",
    "                'updated_at': row.get('updated_at', ''),\n",
    "                'is_suspicious': row.get('is_suspicious', False),\n",
    "                'followers': row.get('followers', 0),\n",
    "                'following': row.get('following', 0),\n",
    "                'commits': row.get('commits', 0),\n",
    "                'commit_list': row.get('commit_list', []),\n",
    "                'public_repos': row.get('public_repos', 0),\n",
    "                'follower_list': row.get('follower_list', []),\n",
    "                'following_list': row.get('following_list', []),\n",
    "                'public_gists': row.get('public_gists', 0),  # Added public_gists\n",
    "                'location': row.get('location', ''),  # Added location\n",
    "                'hirable': row.get('hirable', False),  # Added hirable\n",
    "                'company': row.get('company', ''),  # Added company\n",
    "                'email': row.get('email', ''),\n",
    "                'bio': row.get('bio', ''),\n",
    "                'blog': row.get('blog', ''),\n",
    "                'repo_list': row.get( 'repo_list', [])\n",
    "            }\n",
    "            users.append(user_data)\n",
    "\n",
    "            processed_lines += 1  # Increment after processing a line\n",
    "\n",
    "        if users:\n",
    "            df_users = pd.DataFrame(users)\n",
    "            df_users = df_users.dropna(subset=['login', 'id', 'followers', 'commits'])\n",
    "\n",
    "            write_mode = 'w' if first_chunk else 'a'\n",
    "            # ROBIN: changed to_csv to to_json\n",
    "            df_users.to_json(outfile, mode = write_mode, index=False, lines=True, orient=\"records\")\n",
    "            # df_users.to_csv(outfile, mode=write_mode, index=False, header=first_chunk)\n",
    "            first_chunk = False\n",
    "\n",
    "        if curr > 100 / percent:\n",
    "            break  # Stop if the desired number of lines have been processed\n",
    "\n",
    "        if processed_lines >= lines_to_process:\n",
    "            processed_lines  = 0\n",
    "            curr += 1\n",
    "            outfile = \"split\" + str(curr) + \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcc02e-3e37-4e95-a689-7a1ac18aa093",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitMainDataset(\"dataset.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
